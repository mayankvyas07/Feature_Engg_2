{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cfa04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1. Min-Max scaling:\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale numerical features to a specific range. The scaling is achieved by transforming the data so that the minimum value becomes a specific lower bound (e.g., -1), and the maximum value becomes a specific upper bound (e.g., 1), with values in between being linearly scaled accordingly. This technique is particularly useful when features have different ranges and need to be brought to a common scale.\n",
    "\n",
    "To apply Min-Max scaling to a feature x, the formula is:\n",
    "scaled_x = (x - min_value) / (max_value - min_value) * (upper_bound - lower_bound) + lower_bound\n",
    "\n",
    "Example:\n",
    "Let's say we have a dataset of students' test scores ranging from 60 to 95, and we want to scale them to a range of -1 to 1. The original data looks like this:\n",
    "[60, 75, 80, 85, 95]\n",
    "\n",
    "After applying Min-Max scaling, the data will be transformed to:\n",
    "[-1, -0.333, 0, 0.333, 1]\n",
    "\n",
    "Q2. Unit Vector technique in feature scaling:\n",
    "The Unit Vector technique, also known as normalization or vector normalization, is a feature scaling method that scales the data so that each data point (vector) has a length of 1 (unit length). It involves dividing each data point by its Euclidean norm, which is the square root of the sum of squares of its components.\n",
    "\n",
    "The formula for unit vector scaling of a data point x = [x1, x2, ..., xn] is:\n",
    "unit_x = x / ||x||, where ||x|| is the Euclidean norm of x.\n",
    "\n",
    "Difference from Min-Max scaling:\n",
    "While both Min-Max scaling and Unit Vector technique aim to scale features, they differ in the way they do it. Min-Max scaling scales the data to a specific range, typically between 0 and 1 or -1 and 1, while the Unit Vector technique scales the data to have a length of 1 without enforcing a specific range. As a result, Unit Vector scaling maintains the direction of the original data points while making them unit vectors, which is particularly useful when the direction of the features is important.\n",
    "\n",
    "Example:\n",
    "Consider a dataset with two features, [3, 4] and [5, 12]. We'll apply the Unit Vector technique to these two data points.\n",
    "\n",
    "For the first data point [3, 4]:\n",
    "unit_x1 = [3, 4] / ||[3, 4]|| = [3, 4] / √(3^2 + 4^2) = [0.6, 0.8]\n",
    "\n",
    "For the second data point [5, 12]:\n",
    "unit_x2 = [5, 12] / ||[5, 12]|| = [5, 12] / √(5^2 + 12^2) = [0.384, 0.923]\n",
    "\n",
    "Q3. PCA (Principal Component Analysis) and dimensionality reduction:\n",
    "PCA is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional subspace while preserving most of the original variance in the data. It achieves this by finding the principal components, which are orthogonal vectors that represent the directions of maximum variance in the data. The first principal component explains the largest variance, the second principal component explains the second-largest variance, and so on.\n",
    "\n",
    "The steps to perform PCA are as follows:\n",
    "\n",
    "Standardize the data to have zero mean and unit variance (optional but recommended).\n",
    "Compute the covariance matrix of the standardized data.\n",
    "Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "Sort the eigenvectors in descending order of their corresponding eigenvalues.\n",
    "Select the top k eigenvectors (principal components) that explain the desired amount of variance or reduce the dimensionality to k.\n",
    "Example:\n",
    "Consider a dataset with two features, \"Age\" and \"Income,\" and we want to reduce it to one dimension using PCA. After standardizing the data, we find the eigenvectors and eigenvalues of the covariance matrix:\n",
    "\n",
    "Covariance matrix:\n",
    "| 1.0 0.5 |\n",
    "| 0.5 1.0 |\n",
    "\n",
    "Eigenvalues: 1.5, 0.5\n",
    "Eigenvectors: [0.707, 0.707], [-0.707, 0.707]\n",
    "\n",
    "The first principal component is [0.707, 0.707], and it explains the larger variance (1.5). By projecting the data onto this component, we obtain the reduced one-dimensional representation.\n",
    "\n",
    "Q4. Relationship between PCA and Feature Extraction:\n",
    "PCA can be used for feature extraction because it identifies the most important patterns (principal components) in the data and represents the original features in terms of these components. In other words, it finds a new set of features (the principal components) that are linear combinations of the original features, with each principal component capturing a different aspect of the data's variance.\n",
    "\n",
    "Example of using PCA for Feature Extraction:\n",
    "Let's say we have a dataset with five features: \"A,\" \"B,\" \"C,\" \"D,\" and \"E.\" By applying PCA to this dataset, we can obtain five principal components: PC1, PC2, PC3, PC4, and PC5. Each of these principal components will be a linear combination of the original features, and they will be ranked in order of their importance in explaining the variance in the data.\n",
    "\n",
    "Suppose the principal components are ranked as follows (highest to lowest variance explained): PC1, PC2, PC3, PC4, PC5. If we want to reduce the dimensionality of the dataset while retaining, let's say, 95% of the variance, we can discard the least important components (PC4 and PC5) and use only PC1, PC2, and PC3 as the new feature set for the data.\n",
    "\n",
    "Q5. Using Min-Max scaling for a recommendation system dataset:\n",
    "For the food delivery recommendation system dataset, we have features like price, rating, and delivery time. The goal of Min-Max scaling is to bring all these features to a common scale, typically between 0 and 1.\n",
    "\n",
    "Here's how we would use Min-Max scaling for the dataset:\n",
    "\n",
    "Identify the range of each feature:\n",
    "Price: The minimum price might be $5, and the maximum price could be $50.\n",
    "Rating: The minimum rating could be 1.0, and the maximum rating could be 5.0.\n",
    "Delivery time: The minimum delivery time might be 10 minutes, and the maximum delivery time could be 60 minutes.\n",
    "Apply Min-Max scaling to each feature:\n",
    "scaled_price = (price - 5) / (50 - 5)\n",
    "scaled_rating = (rating - 1.0) / (5.0 - 1.0)\n",
    "scaled_delivery_time = (delivery_time - 10) / (60 - 10)\n",
    "The scaled values will now fall between 0 and 1, making them comparable and suitable for feeding into the recommendation system algorithm.\n",
    "Q6. Using PCA to reduce dimensionality for stock price prediction:\n",
    "In the stock price prediction project, we have a dataset with multiple features, including company financial data and market trends. Dimensionality reduction is useful when dealing with high-dimensional data as it can help improve model performance and reduce computation time.\n",
    "\n",
    "Here's how we would use PCA to reduce the dimensionality of the dataset:\n",
    "\n",
    "Standardize the data:\n",
    "Before applying PCA, it's essential to standardize the data by subtracting the mean and dividing by the standard deviation. Standardizing ensures that all features have equal importance during PCA.\n",
    "Compute the principal components:\n",
    "Calculate the principal components and their corresponding eigenvalues from the standardized data.\n",
    "Sort the principal components:\n",
    "Arrange the principal components in descending order of their corresponding eigenvalues. The components with higher eigenvalues explain more variance in the data.\n",
    "Choose the number of principal components:\n",
    "Decide how many principal components to retain based on the amount of variance explained. It's common to set a threshold (e.g., retaining components that explain 95% of the variance).\n",
    "Project the data onto the selected principal components:\n",
    "Use the selected principal components as a new feature set for the stock price prediction model.\n",
    "By using PCA, we can reduce the number of features while retaining the most important patterns in the data, which can lead to better model performance and interpretability.\n",
    "\n",
    "Q7. Perform Min-Max scaling for the dataset: [1, 5, 10, 15, 20]\n",
    "\n",
    "To transform the values to a range of -1 to 1, we can use the Min-Max scaling formula:\n",
    "\n",
    "scaled_x = (x - min_value) / (max_value - min_value) * (upper_bound - lower_bound) + lower_bound\n",
    "\n",
    "In this case, the minimum value is 1, the maximum value is 20, the lower bound is -1, and the upper bound is 1.\n",
    "\n",
    "Scaled values:\n",
    "scaled_1 = (1 - 1) / (20 - 1) * (1 - (-1)) + (-1) = -1\n",
    "scaled_5 = (5 - 1) / (20 - 1) * (1 - (-1)) + (-1) = -0.6\n",
    "scaled_10 = (10 - 1) / (20 - 1) * (1 - (-1)) + (-1) = -0.2\n",
    "scaled_15 = (15 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0.2\n",
    "scaled_20 = (20 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 1\n",
    "\n",
    "The Min-Max scaled values for the dataset are: [-1, -0.6, -0.2, 0.2, 1]\n",
    "\n",
    "Q8. Performing Feature Extraction using PCA for the dataset: [height, weight, age, gender, blood pressure]\n",
    "\n",
    "When applying PCA for Feature Extraction, we are looking to reduce the dimensionality of the dataset by selecting a smaller set of principal components that capture most of the variance in the data. The number of principal components to retain is determined by the desired level of variance explanation.\n",
    "\n",
    "To decide on the number of principal components to keep, one common approach is to calculate the cumulative explained variance and choose a threshold, e.g., 95%, that determines how much variance is acceptable to retain.\n",
    "\n",
    "Steps to perform PCA for Feature Extraction:\n",
    "\n",
    "Standardize the data: Ensure that all features are on the same scale.\n",
    "Compute the covariance matrix: Calculate the covariance matrix of the standardized data.\n",
    "Calculate eigenvalues and eigenvectors: Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "Sort eigenvalues: Sort the eigenvalues in descending order.\n",
    "Calculate explained variance: Calculate the explained variance for each principal component as the ratio of its eigenvalue to the sum of all eigenvalues.\n",
    "Choose the number of principal components: Select the minimum number of principal components that explain the desired level of variance (e.g., 95%).\n",
    "Let's assume that after performing PCA, we find that the first two principal components explain 80% and 15% of the variance, respectively. We could choose to retain these two principal components as they collectively explain 95% of the variance in the data.\n",
    "\n",
    "By keeping two principal components, we effectively reduce the dataset from five dimensions to two dimensions, making it more manageable for modeling while retaining a significant portion of the original information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
